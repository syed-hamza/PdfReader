[{"id": 1, "messages": [{"sender": "user", "text": "explain backward pass"}, {"sender": "bot", "text": "The backward pass in attention mechanisms applies tiling to equations and involves summing gradients dK and dV across different heads that were implicitly duplicated due to the length N instead of being quadratic. This process is conceptually simpler than the forward pass as it doesn't require softmax rescaling, but implementation-wise it is more involved since there are additional indices into the head for computing the same operation.\n\nA significant advantage of the backward pass in attention models is that it achieves 2-4\u00d7 wall-clock speedup by reducing memory reads/writes. This parallelization scheme is illustrated in Figure 2: <img src=\"./static/Retrievedimages/2307.08691v1/figure-9-4.jpg\"></img>\n\nThe backward pass also uses SRAM to perform matrix operations, which requires more values to be kept compared to the forward pass that only involves 2 matrix multiplications as opposed to 5 in the backward pass due to recomputation. This increases the FLOPs by 2.5 (since there are two matmuls in the forward pass and five in the backward pass).\n\nThe table for this information is not provided, but if available it would be represented as follows:\n<table>\n    <tr>\n        <th>Process</th>\n        <th>Forward Pass FLOPs</th>\n        <th>Backward Pass FLOPs</th>\n    </tr>\n    <tr>\n        <td>Matrix Multiplications</td>\n        <td>2</td>\n        <td>5</td>\n    </tr>\n    <tr>\n        <td>Gradients Summation Across Heads</td>\n        <td>Not Applicable</td>\n        <td>Applicable</td>\n    </tr>\n    <tr>\n        <td>Memory Reads/Writes</td>\n        <td>Higher in Forward Pass</td>\n        <td>Reduced in Backward Pass, leading to Speedup</td>\n    </tr>\n</table>"}]}]